{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Project Final Final version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHhF0ZtKt4IV",
        "colab_type": "text"
      },
      "source": [
        "# Twitter Words Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PeBO-wftV0E",
        "colab_type": "text"
      },
      "source": [
        "By: Gareth Williams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTJcXY5gt16j",
        "colab_type": "code",
        "outputId": "0d4f9d38-e946-44d9-e50f-517380410841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "# storing and anaysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Tokenizers\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Misc. \n",
        "import os\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# SpellChecker\n",
        "!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# TensorFlow \n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "!pip install tensorflow-hub\n",
        "!pip install tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Building the Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: autocorrect in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.18.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (46.0.0)\n",
            "Requirement already satisfied: tfds-nightly in /usr/local/lib/python3.6/dist-packages (2.1.0.dev202003300105)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.3.1.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (3.10.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (19.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.21.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (0.9.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (4.38.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tfds-nightly) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tfds-nightly) (2019.11.28)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tfds-nightly) (46.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tfds-nightly) (1.51.0)\n",
            "Version:  2.2.0-rc1\n",
            "Eager mode:  True\n",
            "Hub version:  0.7.0\n",
            "GPU is NOT AVAILABLE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfJfqd16uAjs",
        "colab_type": "text"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GCzDoYtuGho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spellcheck(input_words):  # Additionally casts to LC and removes excess spaces as a nice side-effect of how it spell-checks.\n",
        "  speller = SpellChecker() # imported from the pyspellcheck library \n",
        "  words = speller.split_words(input_words) # splits sentence into individual words\n",
        "  filtered = [speller.correction( word ) for word in words] # scans through words and spell checks them \n",
        "  filtered = \" \".join(filtered) # joins the individual words together with a space between them\n",
        "\n",
        "  return filtered\n",
        "\n",
        "def remove_stopwords(sent):\n",
        "  # Stop words are words such as “the”, “a”, “an”, “in.” It is useful to remove and ignore these words.\n",
        "  stop_words = set(stopwords.words('english')) # Get the list of stop words of the targeted language\n",
        "  sent = convert(sent) # function made to split the sentences into individual words\n",
        "  filtered = [w for w in sent if not w in stop_words] # scans through sentence and removes Stop Words\n",
        "  filtered = \" \".join(filtered)\n",
        "  return filtered\n",
        "\n",
        "def convert(lst): \n",
        "  try:\n",
        "    return (lst.split())\n",
        "  except AttributeError: # If it cannot split, it returns the list\n",
        "    return lst\n",
        "\n",
        "def remove_hashtag_at(word_tokens):\n",
        "    j = 0\n",
        "    for i in range(len(word_tokens)):\n",
        "        try:\n",
        "            if word_tokens[j] == '@' or word_tokens[j] == '#':\n",
        "                del word_tokens[j + 1]\n",
        "                del word_tokens[j]\n",
        "                j += -1\n",
        "            j += 1\n",
        "        except:\n",
        "            break\n",
        "    return word_tokens\n",
        "\n",
        "def Cleaning(X):\n",
        " # Cleaning data \n",
        "  pop_list = ['#', '@', '*', '\\x89Û_', '\\x89Ûªt', '\\x89', '\\x89ÛÏ', '\\x89Ûªm', '\\x89ÛÓ', '\\x89Ûª', '\\x89Û÷',\n",
        "            '\\x89ÛªS', 'RAZEDåÊ', '\\x89ÛÒ', '...', '?', '|', '_', '-', 'Ûªs', '', '[', ']', '`', '(', ')',\n",
        "            'Û', 'ª','^', '>','0','1','2','3','4','5','6','7','8','9'] # list of things to remove\n",
        "  clean_tweets = [] # empty list\n",
        "  stop_words = set(stopwords.words('english')) # Probably safe to remove this \n",
        "\n",
        "  for i in range(len(X)):\n",
        "    ws = word_tokenize(X[i]) # tokenizes tweet\n",
        "    ws = remove_hashtag_at(ws) # removes #,@ and the names attatched\n",
        "    # Removes websites from tweets (Note: could be improved)\n",
        "    if 'http' in ws:\n",
        "        text = ' '.join(ws[0:ws.index('http')])\n",
        "    elif 'https' in ws:\n",
        "        text = ' '.join(ws[0:ws.index('https')])\n",
        "    else:\n",
        "        text = ' '.join(ws)\n",
        "    for pop in pop_list:\n",
        "        text = text.replace(pop, '')\n",
        "    # text = spellcheck(text) # Corrects tweet spelling (Warning: very, very long run times and I am sure I get better results without it)\n",
        "    text = remove_stopwords(text) # Removes the Stop Words\n",
        "    clean_tweets.append(text) # appends the cleaned tweets \n",
        "\n",
        "\n",
        "  X = clean_tweets # Could just use \"return clean_tweets\" to save memory\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1pQ5Ojkk9w5",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "16DX9KYnDk16",
        "colab": {}
      },
      "source": [
        "# Load in tweet data file\n",
        "data_temp = pd.read_csv('/content/drive/My Drive/DataSets/Twitter Data/Ugly_Words_FULL.csv')\n",
        "data = data_temp.to_numpy() # Convert to Numpy (Trying to do everything using Pandas is a nightmare)\n",
        "data = data[:1500, 0:7] # Pulls all columns and labeled rows\n",
        "data_temp = [] # Free's up memory (using temp incase copying over same variable corrupted anything)\n",
        "\n",
        "# Shuffles the Data\n",
        "np.random.shuffle(data) # Results change because of this. Best manually create balanced set or labels for best results\n",
        "\n",
        "# Cleaning\n",
        "X_Full = data[:1500,6].astype(str) # columns with tweets, and forces all of it to be a strings\n",
        "X_temp = Cleaning(X_Full) # Sends to the Cleaning Function\n",
        "X_Full = X_temp # Just incase a copying error happens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyrdHjT8DeUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train, Validation, and Training set\n",
        "y_train = data[:1000, 5].astype(int) # Forces data to be integers incase it is read as float\n",
        "X_train = X_Full[:1000]\n",
        "y_valid = data[1000:1250, 5].astype(int)\n",
        "X_valid = X_Full[1000:1250]\n",
        "y_test = data[1250:1500, 5].astype(int)\n",
        "X_test = X_Full[1250:1500]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6LTE1bjou8i",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizing, Padding, and Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDWJsgrEoqIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_len = 50 # Length we have to make each tokenized row be (important because Tensor wants everything to be rectangular)\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer() # Sets up the Tokenizer which we will feed\n",
        "\n",
        "tokenizer.fit_on_texts(X_train) # Feed the tokenizer with the training and valid data\n",
        "tokenizer.fit_on_texts(X_valid)\n",
        "# tokenizer.fit_on_texts(X_test) # Get better results from not feeding it the test set\n",
        "                                 # reason might be that it likes being feed new information after training\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train)  # Convert text into numerical values and into vectors\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=token_len, padding='pre', truncating='post') # Pads the vectors so they all are the same length\n",
        "\n",
        "X_valid = tokenizer.texts_to_sequences(X_valid)\n",
        "X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=token_len, padding='pre', truncating='post')\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=token_len, padding='pre', truncating='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU1WlXMPpH5P",
        "colab_type": "text"
      },
      "source": [
        "# Converting Numpy into Tensor friendly arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm8Hofh3oFtx",
        "colab_type": "code",
        "outputId": "f8446e2e-af4a-4d6b-93ba-7fa23d651f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X_train = tf.constant(X_train, dtype=tf.int64) # Constant and some other function can be used to convert to a tensor array\n",
        "y_train = tf.constant(y_train, dtype=tf.int64)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(128).batch(128) # Combines data with labels. (Note: Data then Labels, in that order)\n",
        "\n",
        "X_valid = tf.constant(X_valid, dtype=tf.int64)\n",
        "y_valid = tf.constant(y_valid, dtype=tf.int64)\n",
        "valid_data = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).shuffle(128).batch(128) # .shuffle(128).batch(128) Must use for Tensor to understand\n",
        "\n",
        "X_test = tf.constant(X_test, dtype=tf.int64)\n",
        "y_test = tf.constant(y_test, dtype=tf.int64)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test)).shuffle(128).batch(128)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: ((None, 50), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 50), (None,)), types: (tf.int64, tf.int64)>\n",
            "<BatchDataset shapes: ((None, 50), (None,)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE3EH-vLozZp",
        "colab_type": "text"
      },
      "source": [
        "# Model and Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF2vBIfEoHj6",
        "colab_type": "code",
        "outputId": "92357ea1-d8c2-4675-ea30-7806741ee335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model = tf.keras.Sequential() # Setting up the model\n",
        "model.add(tf.keras.layers.Embedding(len(tokenizer.word_index)+5, 100)) # Adds the embedding the layer into the model ( for \"(len(tokenizer.word_index)+5, 100))\" changing 100 to 1 will squish your data and get poorer results, while expanding to can improve but at a performance costs )\n",
        "model.add(tf.keras.layers.LSTM(100, activation ='relu', return_sequences= False, recurrent_dropout = 0.1)) # Adds LSTM layer into the model. Recurrent_dropout = Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state (so it drops nodes after each iteration, i think)\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # Adds Densely-connected Neural Network layer into the model. Only one node is needed because the sigmoid smooths the outputs from zero to one.\n",
        "print(model.summary()) # prints out summary of the embedded words size and other information on the parameteres. (Note: the larger the Param (a.k.a. unquie words it can store) the longer it takes to train.)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4924\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 100)         492900    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 573,401\n",
            "Trainable params: 573,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfc_Q03oLUz",
        "colab_type": "code",
        "outputId": "d2ecd487-48a5-4e65-87c2-7bf273beacac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # Compile the layers with certain setting we give it. 'adam' is an overall pretty good optimizer. \n",
        "                                                                                  # Binary_Crossentropy is a good loss function for this project because we have binary labels (and in a way, data)\n",
        "history = model.fit(train_data, validation_data=valid_data, epochs=25, verbose=1) # Model Training. Validation Data will help counter overfitting to a point. \n",
        "                                                                                  # Careful with how many epochs used (find a sweet spot)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "8/8 [==============================] - 2s 218ms/step - loss: 0.6809 - accuracy: 0.6810 - val_loss: 0.6556 - val_accuracy: 0.7520\n",
            "Epoch 2/25\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.6205 - accuracy: 0.7240 - val_loss: 0.5537 - val_accuracy: 0.7520\n",
            "Epoch 3/25\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.5809 - accuracy: 0.7240 - val_loss: 0.5821 - val_accuracy: 0.7520\n",
            "Epoch 4/25\n",
            "8/8 [==============================] - 1s 187ms/step - loss: 0.5492 - accuracy: 0.7240 - val_loss: 0.5261 - val_accuracy: 0.7520\n",
            "Epoch 5/25\n",
            "8/8 [==============================] - 1s 184ms/step - loss: 0.4856 - accuracy: 0.7240 - val_loss: 0.5031 - val_accuracy: 0.7520\n",
            "Epoch 6/25\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.4226 - accuracy: 0.7240 - val_loss: 0.4689 - val_accuracy: 0.7520\n",
            "Epoch 7/25\n",
            "8/8 [==============================] - 2s 195ms/step - loss: 0.3252 - accuracy: 0.7360 - val_loss: 0.4282 - val_accuracy: 0.7680\n",
            "Epoch 8/25\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.2391 - accuracy: 0.8850 - val_loss: 0.3997 - val_accuracy: 0.8360\n",
            "Epoch 9/25\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 2.1124 - accuracy: 0.9300 - val_loss: 0.4764 - val_accuracy: 0.8120\n",
            "Epoch 10/25\n",
            "8/8 [==============================] - 1s 187ms/step - loss: 0.3100 - accuracy: 0.9840 - val_loss: 0.4970 - val_accuracy: 0.8280\n",
            "Epoch 11/25\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.2911 - accuracy: 0.9860 - val_loss: 0.4867 - val_accuracy: 0.8320\n",
            "Epoch 12/25\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.2056 - accuracy: 0.9880 - val_loss: 0.5111 - val_accuracy: 0.7920\n",
            "Epoch 13/25\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.3536 - accuracy: 0.9830 - val_loss: 0.5203 - val_accuracy: 0.8360\n",
            "Epoch 14/25\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.0767 - accuracy: 0.9890 - val_loss: 0.4507 - val_accuracy: 0.8360\n",
            "Epoch 15/25\n",
            "8/8 [==============================] - 1s 181ms/step - loss: 0.0780 - accuracy: 0.9900 - val_loss: 0.4491 - val_accuracy: 0.8320\n",
            "Epoch 16/25\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.0698 - accuracy: 0.9910 - val_loss: 0.4554 - val_accuracy: 0.8360\n",
            "Epoch 17/25\n",
            "8/8 [==============================] - 1s 185ms/step - loss: 0.0595 - accuracy: 0.9910 - val_loss: 0.4933 - val_accuracy: 0.8240\n",
            "Epoch 18/25\n",
            "8/8 [==============================] - 1s 180ms/step - loss: 0.0505 - accuracy: 0.9920 - val_loss: 0.7446 - val_accuracy: 0.8240\n",
            "Epoch 19/25\n",
            "8/8 [==============================] - 1s 181ms/step - loss: 0.0445 - accuracy: 0.9930 - val_loss: 4.5709 - val_accuracy: 0.8240\n",
            "Epoch 20/25\n",
            "8/8 [==============================] - 1s 179ms/step - loss: 0.0386 - accuracy: 0.9930 - val_loss: 14.3615 - val_accuracy: 0.8240\n",
            "Epoch 21/25\n",
            "8/8 [==============================] - 1s 180ms/step - loss: 0.0326 - accuracy: 0.9930 - val_loss: 32.0113 - val_accuracy: 0.8320\n",
            "Epoch 22/25\n",
            "8/8 [==============================] - 1s 176ms/step - loss: 0.0636 - accuracy: 0.9910 - val_loss: 45.6978 - val_accuracy: 0.8240\n",
            "Epoch 23/25\n",
            "8/8 [==============================] - 1s 180ms/step - loss: 0.0379 - accuracy: 0.9930 - val_loss: 27.4061 - val_accuracy: 0.8200\n",
            "Epoch 24/25\n",
            "8/8 [==============================] - 1s 180ms/step - loss: 0.0376 - accuracy: 0.9920 - val_loss: 19.9389 - val_accuracy: 0.8200\n",
            "Epoch 25/25\n",
            "8/8 [==============================] - 1s 184ms/step - loss: 0.0378 - accuracy: 0.9930 - val_loss: 16.0612 - val_accuracy: 0.8200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytj0gimMK4Dp",
        "colab_type": "code",
        "outputId": "95a8c29e-3a70-40c1-b3fe-f5390fca887f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Running the Test data\n",
        "results = model.evaluate(X_test, y_test, batch_size=128) # Prints out the results\n",
        "\n",
        "# Print out the results\n",
        "for name, value in zip(model.metrics_names, results): # Prints out the same results without the progress bar\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5029 - accuracy: 0.8600\n",
            "loss: 0.503\n",
            "accuracy: 0.860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lLwNMG3ODwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights\n",
        "# model.save_weights('Good_weights.h5', overwrite=True) # un-hastag if the accuracy is above 86% (loss: 0.503) \n",
        "                                                        # Need to balance the training data (and maybe oversample) in order to break the 90%"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uWIw6J6ePkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}